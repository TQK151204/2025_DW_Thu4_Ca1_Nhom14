import requests
from bs4 import BeautifulSoup
from datetime import datetime
import csv
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random

# =========================
# ‚ö° C·∫•u h√¨nh
# =========================
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
}

CATEGORIES = [
    "https://hoanghamobile.com/dien-thoai-di-dong/iphone/iphone-13-series",
    "https://hoanghamobile.com/dien-thoai-di-dong/iphone/iphone-14-series",
    "https://hoanghamobile.com/dien-thoai-di-dong/iphone/iphone-15-series",
    "https://hoanghamobile.com/dien-thoai-di-dong/iphone/iphone-16-series",
    "https://hoanghamobile.com/dien-thoai-di-dong/iphone/iphone-17-series"
]

OUTPUT_DIR = "StagingArea/crawl_data/hoanghamobile"
os.makedirs(OUTPUT_DIR, exist_ok=True)


# =========================
# üîó T·∫£i HTML v·ªõi retry v√† delay
# =========================
def get_html(url, retries=4, timeout=25):
    for attempt in range(1, retries + 1):
        try:
            print(f"üîó ƒêang t·∫£i: {url} (l·∫ßn {attempt})")
            response = requests.get(url, headers=HEADERS, timeout=timeout)
            response.raise_for_status()
            
            # Ngh·ªâ ng·∫´u nhi√™n 1‚Äì2 gi√¢y ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
            time.sleep(random.uniform(1.0, 2.0))
            return response.text

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói t·∫£i trang {url} (l·∫ßn {attempt}): {e}")
            if attempt < retries:
                time.sleep(2)
            else:
                return None


# =========================
# üì¶ Parse 1 s·∫£n ph·∫©m
# =========================
def parse_item(item, crawl_datetime):
    """Tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m theo chu·∫©n"""
    try:
        source_name = "Hoanghamobile"

        # T√™n s·∫£n ph·∫©m
        name_tag = item.select_one("h3.product-name") or item.select_one("h3 a")
        name = name_tag.text.strip() if name_tag else "Kh√¥ng r√µ t√™n"

        # Gi√° c≈©
        price_old_tag = item.select_one("div.price.price-last strike")
        price_old = price_old_tag.text.strip().replace("‚Ç´", "").replace(".", "") if price_old_tag else "0"

        # Gi·∫£m gi√°
        discount_tag = item.select_one("div.price.price-last span")
        discount = discount_tag.text.strip().replace("-", "") if discount_tag else "0"

        # Gi√° hi·ªán t·∫°i
        price_now_tag = item.select_one("div.price strong")
        price_now = price_now_tag.text.strip().replace("‚Ç´", "").replace(".", "") if price_now_tag else "0"

        # ·∫¢nh s·∫£n ph·∫©m (l·ªçc ·∫£nh ch√≠nh)
        image_url = ""
        for img_tag in item.select("img"):
            img_src = img_tag.get("src") or img_tag.get("data-src") or ""
            if img_src.startswith("/"):
                img_src = "https://hoanghamobile.com" + img_src
            if "/Uploads/" in img_src and "sticker" not in img_src.lower() and "icon" not in img_src.lower():
                image_url = img_src
                break

        # Link s·∫£n ph·∫©m
        link_tag = item.select_one("a")
        product_url = link_tag.get("href", "") if link_tag else ""
        if product_url and not product_url.startswith("http"):
            product_url = "https://hoanghamobile.com" + product_url

        return {
            "product_name": name,
            "brand_name": "Apple",
            "price": price_now,
            "old_price": price_old,
            "discount_percent": discount,
            "image_url": image_url,
            "product_url": product_url,
            "source_name": source_name,
            "source_url": "https://hoanghamobile.com",
            "crawl_date": crawl_datetime.strftime("%Y-%m-%d"),
            "crawl_time": crawl_datetime.strftime("%H:%M:%S"),
        }
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói parse s·∫£n ph·∫©m: {e}")
        return None


# =========================
# üõí Crawl 1 danh m·ª•c
# =========================
def crawl_category(url):
    print(f"‚è≥ Crawl danh m·ª•c: {url}")
    html = get_html(url)
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")
    items = soup.select("div.pj16-item")
    print(f"üîç T√¨m th·∫•y {len(items)} s·∫£n ph·∫©m")

    crawl_datetime = datetime.now()
    products = [parse_item(item, crawl_datetime) for item in items if parse_item(item, crawl_datetime)]
    return products


# =========================
# üèÉ Crawl nhi·ªÅu danh m·ª•c (multi-thread)
# =========================
def crawl_hoanghamobile(url_list):
    all_products = []

    with ThreadPoolExecutor(max_workers=5) as executor:
        tasks = [executor.submit(crawl_category, url) for url in url_list]
        for task in as_completed(tasks):
            all_products.extend(task.result())

    return all_products


# =========================
# üíæ L∆∞u d·ªØ li·ªáu CSV
# =========================
def save_to_csv_hoanghamobile(data, output_dir):
    if not data:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u CSV")
        return None

    os.makedirs(output_dir, exist_ok=True)
    filename = f"HoangHaMobile_Product_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv"
    filepath = os.path.join(output_dir, filename)

    with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=list(data[0].keys()))
        writer.writeheader()
        writer.writerows(data)

    print(f"üíæ L∆∞u file CSV th√†nh c√¥ng: {filepath}")
    return filepath


# =========================
# üîπ Test tr·ª±c ti·∫øp
# =========================
if __name__ == "__main__":
    products = crawl_hoanghamobile(CATEGORIES)
    print(f"‚úÖ T·ªïng s·ªë s·∫£n ph·∫©m: {len(products)}")
    save_to_csv_hoanghamobile(products, OUTPUT_DIR)
