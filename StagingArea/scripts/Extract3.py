import requests
from bs4 import BeautifulSoup
from datetime import datetime
import csv
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

# ============================================
# üõ° C·∫•u h√¨nh headers
# ============================================
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
}

# ============================================
# üåê Danh s√°ch URL c√°c series iPhone tr√™n TGDD
# ============================================
CATEGORIES = [
    "https://www.thegioididong.com/dtdd-apple-iphone-13-series",
    "https://www.thegioididong.com/dtdd-apple-iphone-14-series",
    "https://www.thegioididong.com/dtdd-apple-iphone-15-series",
    "https://www.thegioididong.com/dtdd-apple-iphone-16-series",
    "https://www.thegioididong.com/dtdd-apple-iphone-air",
    "https://www.thegioididong.com/dtdd-apple-iphone-17-series"
]

# Th∆∞ m·ª•c l∆∞u CSV
OUTPUT_DIR = "StagingArea/crawl_data/tgdd"
os.makedirs(OUTPUT_DIR, exist_ok=True)


# ============================================
# üîπ T·∫£i HTML t·ª´ URL
# ============================================
def get_html(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói t·∫£i trang {url}: {e}")
        return None


# ============================================
# üîπ Parse 1 s·∫£n ph·∫©m
# ============================================
def parse_item(item, crawl_datetime):
    """Tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m theo ƒë·ªãnh d·∫°ng chu·∫©n"""
    try:
        # T√™n s·∫£n ph·∫©m
        name_tag = item.select_one("h3.box-title") or item.select_one("h3")
        name = name_tag.text.strip() if name_tag else "Kh√¥ng r√µ t√™n"

        # Gi√° hi·ªán t·∫°i
        price_now_tag = item.select_one("strong.price")
        price_now = price_now_tag.text.strip().replace(".", "").replace("‚Ç´", "") if price_now_tag else "0"

        # Gi√° c≈©
        price_old_tag = item.select_one("p.price-old.black")
        price_old = price_old_tag.text.strip().replace(".", "").replace("‚Ç´", "") if price_old_tag else "0"

        # Gi·∫£m gi√°
        discount_tag = item.select_one("span.percent")
        discount = discount_tag.text.strip().replace("-", "") if discount_tag else "0"

        # ·∫¢nh
        img_tag = item.select_one("img")
        image_url = img_tag.get("src") or img_tag.get("data-src") if img_tag else ""

        # Link s·∫£n ph·∫©m
        link_tag = item.select_one("a")
        product_url = link_tag.get("href", "") if link_tag else ""
        if product_url and not product_url.startswith("http"):
            product_url = "https://www.thegioididong.com" + product_url

        return {
            "product_name": name,
            "brand_name": "Apple",
            "price": price_now,
            "old_price": price_old,
            "discount_percent": discount,
            "image_url": image_url,
            "product_url": product_url,
            "source_name": "Thegioididong",
            "source_url": "https://www.thegioididong.com",
            "crawl_date": crawl_datetime.strftime("%Y-%m-%d"),
            "crawl_time": crawl_datetime.strftime("%H:%M:%S"),
        }
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói parse s·∫£n ph·∫©m: {e}")
        return None


# ============================================
# üîπ Crawl 1 danh m·ª•c
# ============================================
def crawl_category(url):
    print(f"‚è≥ Crawl danh m·ª•c: {url}")
    html = get_html(url)
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")
    items = soup.select("ul.listproduct li")
    print(f"üîç T√¨m th·∫•y {len(items)} s·∫£n ph·∫©m")

    crawl_datetime = datetime.now()
    products = [parse_item(item, crawl_datetime) for item in items if parse_item(item, crawl_datetime)]
    return products


# ============================================
# üîπ Crawl nhi·ªÅu danh m·ª•c (multi-thread)
# ============================================
def crawl_tgdd(url_list):
    all_products = []

    with ThreadPoolExecutor(max_workers=6) as executor:
        tasks = [executor.submit(crawl_category, url) for url in url_list]
        for task in as_completed(tasks):
            all_products.extend(task.result())

    return all_products


# ============================================
# üîπ L∆∞u d·ªØ li·ªáu ra CSV
# ============================================
def save_to_csv_tgdd(data, output_dir):
    if not data:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u CSV")
        return None

    os.makedirs(output_dir, exist_ok=True)
    filename = f"TGDD_Product_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv"
    filepath = os.path.normpath(os.path.join(output_dir, filename))

    with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=list(data[0].keys()))
        writer.writeheader()
        writer.writerows(data)

    print(f"üíæ L∆∞u file CSV th√†nh c√¥ng: {filepath.replace(os.sep, '/')}")
    return filepath


# ============================================
# üîπ TEST TR·ª∞C TI·∫æP
# ============================================
if __name__ == "__main__":
    products = crawl_tgdd(CATEGORIES)
    print(f"‚úÖ T·ªïng s·ªë s·∫£n ph·∫©m: {len(products)}")
    save_to_csv_tgdd(products, OUTPUT_DIR)
